# Stable Diffusion with VQ-VAE and U-Net

Thid project aims to build and train a stable diffusion model with VQ-VAE + U-Net architecture. The objective is to familiarize myself with the PyTorch lightning + Hydra + Weights & Biases workflow. PyTorch lightning to handle scaled modularized training, Hydra for managing configuration and Weights & Biases for logging, storing, and hyperparameter optimization (WandB Sweep). 

## Table of Contents

- [Project Overview](#project-overview)
- [Technologies Used](#technologies-used)
- [Setup and Installation](#setup-and-installation)
- [Training](#training)
  - [Training VQ-VAE](#training-vq-vae)
  - [Training U-Net Diffusion Model](#training-u-net-diffusion-model)
- [Hyperparameter Optimization](#hyperparameter-optimization)
  - [VQ-VAE Hyperparameter Optimization](#vq-vae-hyperparameter-optimization)
  - [U-Net Diffusion Model Hyperparameter Optimization](#u-net-diffusion-model-hyperparameter-optimization)

## Project Overview

The project requires training two models:

1.  **VQ-VAE Training:** Initially, VQ-VAE is trained to learn a latent space with less dimensions that the original image data. It looks to compress 3x128x128 image into a 6x32x32 latent image for a more efficient computation friendly diffusion training. 
2.  **Diffusion Model Training:** With the pre-trained VQ-VAE, the U-Net learns to generate the latent image distribution. This is done through learning the denoising progress of approximately thousands of steps in the noising process. This allows generation/extrapolation of new data samples from the dataset (CelebHQ dataset)

## Tools Used

*   **Python:** Main programming language.
*   **PyTorch:** Deep learning framework.
*   **PyTorch Lightning:** Modularizes and simplifies PyTorch code, while enabling modern standards in state of the art model training. 
*   **Hydra:** Allows configuration management and command line control.
*   **Weights & Biases (WandB):** For logging, visualization, and hyperparameter sweeps.

## Setup and Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/KCTaka/Stable-Diffusion-from-Scratch.git
    cd Stable-Diffusion-from-Scratch
    ```

2.  **Create and activate a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate # On Windows use `venv\Scripts\activate`
    ```

3.  **Install dependencies:**
    (Assuming you have a `requirements.txt` file)
    ```bash
    pip install -r requirements.txt
    ```
    Alternatively, if you are managing dependencies with Conda and an `environment.yml` file:
    ```bash
    conda env create -f environment.yml
    conda activate <your-env-name>
    ```

4.  **Set up WandB:**
    Log in to your WandB account:
    ```bash
    wandb login
    ```

## Training

This project will use the CelebHQ 
Dataset used: [CelebA-HQ Resized (256x256)](https://www.kaggle.com/datasets/badasstechie/celebahq-resized-256x256) by Moses Odhiambo on Kaggle.

### Training VQ-VAE

To train the VQ-VAE model, run the following command from the root of the project:

```bash
python src/train.py
```

Configuration for the VQ-VAE training (e.g., model parameters, dataset paths, training hyperparameters) can be managed through Hydra configuration files, located in a `configs` directory.

### Training U-Net Diffusion Model

After training the VQ-VAE and obtaining a latent space representation, train the U-Net based diffusion model using:

```bash
python src/train_ddpm.py
```

This script will train the U-Net on the latent space data generated by the pre-trained VQ-VAE. Configurations for this stage are also managed by Hydra under the configs directory.

## Hyperparameter Optimization

This project utilizes Weights & Biases Sweeps for hyperparameter optimization.

### VQ-VAE Hyperparameter Optimization

Hyperparameter optimization for the VQ-VAE model:

1.  **Initialize the sweep:**
    ```bash
    wandb sweep --project <project-name> configs/sweep/wandb_ae.yaml
    ```
    This command will output a sweep ID.

2.  **Run the WandB agent(s):**
     `<sweep-id>` with the ID obtained from the previous step.
    ```bash
    wandb agent <team-name>/<project-name>/<sweep-id>
    ```
    You can run multiple agents in parallel (e.g., on different machines or GPUs) to speed up the optimization process.

### U-Net Diffusion Model Hyperparameter Optimization

Hyperparameter optimization for the U-Net diffusion model:

1.  **Initialize the sweep:**
    ```bash
    wandb sweep --project <project-name> configs/sweep/wandb_ddpm.yaml
    ```
    This command will output a sweep ID.

2.  **Run the WandB agent(s):**
`<sweep-id>` with the ID obtained from the previous step.
    ```bash
    wandb agent <team-name>/<project-name>/<sweep-id>
    ```
