{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Stable Diffusion with VQ-VAE and U-Net","text":"<p>Thid project aims to build and train a stable diffusion model with VQ-VAE + U-Net architecture. The objective is to familiarize myself with the PyTorch lightning + Hydra + Weights &amp; Biases workflow. PyTorch lightning to handle scaled modularized training, Hydra for managing configuration and Weights &amp; Biases for logging, storing, and hyperparameter optimization (WandB Sweep). </p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Project Overview</li> <li>Technologies Used</li> <li>Setup and Installation</li> <li>Training</li> <li>Training VQ-VAE</li> <li>Training U-Net Diffusion Model</li> <li>Hyperparameter Optimization</li> <li>VQ-VAE Hyperparameter Optimization</li> <li>U-Net Diffusion Model Hyperparameter Optimization</li> </ul>"},{"location":"#project-overview","title":"Project Overview","text":"<p>The project requires training two models:</p> <ol> <li>VQ-VAE Training: Initially, VQ-VAE is trained to learn a latent space with less dimensions that the original image data. It looks to compress 3x128x128 image into a 6x32x32 latent image for a more efficient computation friendly diffusion training. </li> <li>Diffusion Model Training: With the pre-trained VQ-VAE, the U-Net learns to generate the latent image distribution. This is done through learning the denoising progress of approximately thousands of steps in the noising process. This allows generation/extrapolation of new data samples from the dataset (CelebHQ dataset)</li> </ol>"},{"location":"#tools-used","title":"Tools Used","text":"<ul> <li>Python: Main programming language.</li> <li>PyTorch: Deep learning framework.</li> <li>PyTorch Lightning: Modularizes and simplifies PyTorch code, while enabling modern standards in state of the art model training. </li> <li>Hydra: Allows configuration management and command line control.</li> <li>Weights &amp; Biases (WandB): For logging, visualization, and hyperparameter sweeps.</li> </ul>"},{"location":"#setup-and-installation","title":"Setup and Installation","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/KCTaka/Stable-Diffusion-from-Scratch.git\ncd Stable-Diffusion-from-Scratch\n</code></pre></p> </li> <li> <p>Create and activate a virtual environment (recommended): <pre><code>python -m venv venv\nsource venv/bin/activate # On Windows use `venv\\Scripts\\activate`\n</code></pre></p> </li> <li> <p>Install dependencies: <pre><code>pip install -r requirements.txt\n</code></pre>     Alternatively, if you are managing dependencies with Conda and an <code>environment.yml</code> file:     <pre><code>conda env create -f environment.yml\nconda activate &lt;your-env-name&gt;\n</code></pre></p> </li> <li> <p>Set up WandB:     Log in to your WandB account:     <pre><code>wandb login\n</code></pre></p> </li> </ol>"},{"location":"#training","title":"Training","text":"<p>This project will use the CelebHQ  Dataset used: CelebA-HQ Resized (256x256) by Moses Odhiambo on Kaggle.</p>"},{"location":"#training-vq-vae","title":"Training VQ-VAE","text":"<p>To train the VQ-VAE model, run the following command from the root of the project:</p> <pre><code>python src/train.py\n</code></pre> <p>Configuration for the VQ-VAE training (e.g., model parameters, dataset paths, training hyperparameters) can be managed through Hydra configuration files, located in a <code>configs</code> directory.</p>"},{"location":"#training-u-net-diffusion-model","title":"Training U-Net Diffusion Model","text":"<p>After training the VQ-VAE and obtaining a latent space representation, train the U-Net based diffusion model using:</p> <pre><code>python src/train_ddpm.py\n</code></pre> <p>This script will train the U-Net on the latent space data generated by the pre-trained VQ-VAE. Configurations for this stage are also managed by Hydra under the configs directory.</p>"},{"location":"#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<p>This project utilizes Weights &amp; Biases Sweeps for hyperparameter optimization.</p>"},{"location":"#vq-vae-hyperparameter-optimization","title":"VQ-VAE Hyperparameter Optimization","text":"<p>Hyperparameter optimization for the VQ-VAE model:</p> <ol> <li> <p>Initialize the sweep: <pre><code>wandb sweep --project &lt;project-name&gt; configs/sweep/wandb_ae.yaml\n</code></pre>     This command will output a sweep ID.</p> </li> <li> <p>Run the WandB agent(s): <code>&lt;sweep-id&gt;</code> with the ID obtained from the previous step.     <pre><code>wandb agent &lt;team-name&gt;/&lt;project-name&gt;/&lt;sweep-id&gt;\n</code></pre>     You can run multiple agents in parallel (e.g., on different machines or GPUs) to speed up the optimization process.</p> </li> </ol>"},{"location":"#u-net-diffusion-model-hyperparameter-optimization","title":"U-Net Diffusion Model Hyperparameter Optimization","text":"<p>Hyperparameter optimization for the U-Net diffusion model:</p> <ol> <li> <p>Initialize the sweep: <pre><code>wandb sweep --project &lt;project-name&gt; configs/sweep/wandb_ddpm.yaml\n</code></pre>     This command will output a sweep ID.</p> </li> <li> <p>Run the WandB agent(s): <code>&lt;sweep-id&gt;</code> with the ID obtained from the previous step.     <pre><code>wandb agent &lt;team-name&gt;/&lt;project-name&gt;/&lt;sweep-id&gt;\n</code></pre></p> </li> </ol>"},{"location":"reflection/","title":"Project Comments and Reflection","text":""},{"location":"reflection/#intention","title":"Intention","text":"<p>I had this grand idea. I occasionally create edit/videos with cartoons/anime and realized its incompatibility with slow-motion. There are methods online to use an algorithm (Twixtor) to extrapolate and increase frame rate; however, there was two huge problem with that: 1. Twixtor bad job with interpolating between two frames (lacks understanding of the object in motion) 2. the video frame rate does not match with the true animation frame rate </p> <p>With the ever-growing advancement of generation models, and the passion/intention to develop my skills as an engineer, I decided to build my own model to approach the proposed two challenges. This is a large leap for me, and I must acquire - along with many others - the following skills:  </p> <ul> <li>Design an appropriate model architecture  </li> <li>Manage and organize datasets  </li> <li>Train efficiently with appropriate techniques  </li> </ul> <p>I've had basic skills with PyTorch and distributed training with torch run but I struggled on finding the right hyperparameter configuration which often led to large time consumption trying to figure out why it seemingly didn't work when the code itself was fine. I decided to use sweeping (hyperparameter optimization) and proper modern MLOps standards to improve my workflow for the big project.</p>"},{"location":"reflection/#what-i-liked-to-do","title":"What I liked to do","text":"<p>I've always loved and have been doing things from scratch to a point where I am incapable of learning something without understanding its seemingly unnecessary basics. Many languages, libraries, and dependencies are a black bock in the sense that you as the user do not need to know how it happens but only what it does. However, when I plan what to do with this dependency, I tend to need to know how it does what it does so that it does not conflict with what I plan to make out of it. This may sound proper, but I get overly cautious and am not able to proceed without researching and understanding how it works. This project is a large step towards finding that balance and making progress despite not fully understanding what happens under the hood. I believe this is necessary to some extent when working on a large project where it's near impossible to grasp everything before proceeding, leading to huge time loss. </p>"},{"location":"reflection/#in-the-beginning","title":"In the beginning","text":"<p>At the start, planned to do this project with PyTorch + torch run + tensorboard + Optuna. Which went okay but the code looked really messy. That's when I discovered using something like PyTorch lightning and W&amp;B is the current standard in the ML workflow. I had to revamp the entire project to accommodate the lightning structure and W&amp;B logging. However, with that, it helped me understand Optuna, tensorboard, and torch run much more than before. This was my vague realization (although Optuna offers more in sweeping):  </p> <ul> <li>W&amp;B = Optuna + Tensorboard </li> <li>PyTorch Lightning = PyTorch + torch run </li> </ul>"},{"location":"reflection/#problems","title":"Problems","text":"<p>There were so many problems that just wouldn't go my way:  </p> <ul> <li> <p>Discovering PyTorch lightning.   I discovered that people use lightning instead of what I had for simplicity, and I knew about it after going through hell to write a messy yet functional distributed training code. I think the majority of my time was wasted from this.  </p> </li> <li> <p>W%B sweep and agent does not programmatically work well with lightning   The problem was multiple GPU runs the agent and sweep code making many sweeps as there is GPU. I could not find solution for this, I just had to avoid it with command line sweep method.  </p> </li> <li> <p>Time-embedding required a convolutional layer in two stages before adding it into the model flow.   One for in general and the other for each block/layer in the model architecture. I initially had just one for each which led to inconsistent and irregular generation patterns.  </p> </li> <li> <p>W%B Model logging/storing stored ALL model after every epoch   I was surprised I couldn't find a single solution to this simple and intuitively common problem. I ended up just giving up and just storing the latest model only.  </p> </li> <li> <p>U-net architecture had to be built in a certain way than what I initially thought.   As the paper and videos I was watching had interesting blocks, the way to skip connection in U-net was different from my previous understanding of how u-net looked like. The concatenation happened after down-sampling, whereas the standard u-net structure had before down-sampling. Also downsampling was done with Conv2d instead of MaxPooling, which probably allowed the model to pick up on more details but introduced more parameters to train (since this happens only 6 times in total, I deem it insignificant).  </p> </li> </ul> <p>Each of the above problems (and many more) just led to me not being able to progress for so long. </p> <p>Another problem I just encountered is that the model didn't save after completion on W&amp;B for some reason and I lost progress on 100 epochs. At least I had the logging saved, but I would have to train from scratch again if I wanteed the model file. Also, I forgot to log the quantized latent image after all denoising process during validation phase.</p>"},{"location":"reflection/#comments","title":"Comments","text":"<p>Although I hated how even the simple things I couldn't progress, the outcome was better than what I followed on paper. This is definitely a large step towards the big project</p>"},{"location":"report/","title":"Stable Diffusion Report","text":"<p>In stable diffusion, you must train two different components, the autoencoder and the diffusion model. </p> <ul> <li>Autoencoder to compress the high resolution image into something less computationally required for generation later on. </li> <li>Diffusion model to learnt to generate an image in the latent space from noise.  This report goes over the history and progress of generating a non-existing image. </li> </ul> <p>For each models, the following general steps were followed:</p> <ol> <li>Find a set of hyperparameter configuration that generally works on about 30 epochs. </li> <li>Hyperparameter Optimize (Sweep) centered around that found hyperparameter configuration. The target loss after 10 epochs is recorded (with the exception of early stopping pruning)</li> <li>The optimized configuration is trained for 50 or 100 epochs.</li> </ol>"},{"location":"report/#autoencoder","title":"AutoEncoder","text":"<p>Vector-Quantized Variational Auto-Encoder (VQVAE) was used to convert images into smaller sized latent images. VQVAE leveraged codebook look up that discretized latent values and allowed a more representative latent space of the original image. The autoencoder first encodes the image into a pre-quantized latent image, quantize through looking-up pixel-embedding to the nearest codebook embedding, and decode the post-quantized latent image where the losses are calculated. To ensure the pre-quantized pixel-embedding does not stray too far from the embedding, codebook+commitment losses are calculated and backpropagated along with the image-related losses.  </p>"},{"location":"report/#hyperparameter-optimization","title":"Hyperparameter Optimization:","text":"Sweep Configurations (Hyperparameter range) <pre><code>program: \"src/train_ddpm.py\"\nname: sweep_ddpm\nmethod: bayes\nmetric:\nname: \"Validation/Loss-ddpm\"\ngoal: minimize\n\nparameters:\nmodels.diffusion.down_channels:\n    values:\n    - [256, 384, 512, 768]\n    - [128, 256, 512, 768]\n    - [64, 128, 512, 768]\nmodels.diffusion.mid_channels:\n    values:\n    - [768, 512]\n    - [768, 512, 512]\nmodels.diffusion.t_emb_dim:\n    distribution: q_log_uniform_values\n    min: 128\n    max: 1024\nmodels.diffusion.num_down_layers:\n    distribution: int_uniform\n    min: 1\n    max: 4\nmodels.diffusion.num_mid_layers:\n    distribution: int_uniform\n    min: 1\n    max: 4\nmodels.diffusion.num_up_layers:\n    distribution: int_uniform\n    min: 1\n    max: 4\nmodels.diffusion.num_heads:\n    values: [2, 4, 8, 16, 32]\nmodels.diffusion.conv_out_channels:\n    distribution: q_log_uniform_values\n    min: 32\n    max: 512\n    q: 32\n\nnoise_scheduler.num_timesteps:\n    distribution: q_log_uniform_values\n    min: 500\n    max: 3000\nnoise_scheduler.beta_start:\n    distribution: log_uniform_values\n    min: 1e-5\n    max: 1e-3\nnoise_scheduler.beta_end:\n    distribution: log_uniform_values\n    min: 1e-3\n    max: 1e-1\nddpm.lr:\n    distribution: log_uniform_values\n    min: 1e-6\n    max: 1e-3\ndatamodule.batch_size:\n    distribution: q_log_uniform_values\n    min: 16\n    max: 128\n\nearly_terminate:\ntype: \"hyperband\"\nmin_iter: 3\n\ncommand:\n- ${env}\n- ${interpreter}\n- ${program}\n- ${args_no_hyphens}\n</code></pre>"},{"location":"report/#adversarial-losses","title":"Adversarial Losses","text":""},{"location":"report/#codebook-and-commitment-losses","title":"Codebook and Commitment Losses","text":""},{"location":"report/#perceptual-losses","title":"Perceptual Losses","text":""},{"location":"report/#reconstruction-losses","title":"Reconstruction Losses","text":""},{"location":"report/#notable-sweep-configurations","title":"Notable Sweep Configurations","text":"<p>The following are some notable directions the vqvae began to learn the dataset to latent space conversion.</p>"},{"location":"report/#run-1-2025-06-05_02-06-18","title":"Run 1: 2025-06-05_02-06-18","text":"<p>Comparing the original image (top row), latent image (middle row), reconstructed image (bottom row) for run 1: 2025-06-05_02-06-18. Epoch: 10</p> Parameter Configurations (Run 1) <pre><code>datamodule:\n  batch_size: 16\nmodels:\n  autoencoder:\n    beta: 0.031434697351974974\n    down_channels: [16, 64, 128, 256]\n    embedding_dim: 6\n    num_down_layers: 4\n    num_mid_layers: 1\n    num_up_layers: 1\n    num_embeddings: 3708\n    num_heads: 4\n  discriminator:\n    conv_channels: [16, 32, 64]\nsystems:\n  loss_weights:\n    adversarial: 0.02277441690906937\n    internal: 18.28967516429656\n    perceptual: 0.0002447386292215411\n    reconst: 0.14919996447016615\nlr:\n  d: 3.6026788983424064e-06\n  g: 3.6362412179266015e-05\n</code></pre> <p>It can be noted that the reconstructed images in this run displays a generic face failing to generate any unique features for each image. The output shows a 'mean' or 'averaged' face, common when the autoencoder is not able to capture distinct features and resort to a single averaged face where the loss is the lowest for all images. </p> <p>A reason for this failure is likely due to the high internal loss weight of 18.29, compared to the other loss weights of adversarial, perceptual and reconstruction where they are 0.023, 0.00024, and 0.15 respectively. The dominant internal loss (the sum of codebook and commitment loss with beta=0.03) renders the visual losses negligible which focuses the autoencoder in learning a stable yet meaningless codebook. This as a result teaches the model to produce a safe and averaged image to the overly stablized latent space.  </p>"},{"location":"report/#run-2-2025-06-05_03-23-41","title":"Run 2: 2025-06-05_03-23-41","text":"<p>Comparing the original image (top row), latent image (middle row), reconstructed image (bottom row) for run 2: 2025-06-05_03-23-41. Epoch: 10</p> Parameter Configurations (Run 2) <pre><code>datamodule:\n  batch_size: 16\nmodels:\n  autoencoder:\n    beta: 0.07830023604089383\n    down_channels: [16, 64, 128, 256]\n    embedding_dim: 6\n    num_down_layers: 4\n    num_mid_layers: 2\n    num_up_layers: 2\n    num_embeddings: 2327\n    num_heads: 2\n  discriminator:\n    conv_channels: [64, 128, 256, 512]\nsystems:\n  loss_weights:\n    adversarial: 0.00008909160625646387\n    internal: 0.0823042928039229\n    perceptual: 0.002681812647958391\n    reconst: 90.66739309587147\nlr:\n  d: 3.637625577905393e-06\n  g: 3.637665086781343e-05\n</code></pre> <p>Run 2 shows improvement in reconstruction quality in-comparison to the previous run 1. The output images are relatively recognizable - showing uniquness to a certain extent - indicating the latent space holds a form of original image information. With that said, the reconstructed images lacks detail and appears to be blurry. The latent images displays extreme colors which could suggest the models inability to encode and compress the image in a meaningful manner. </p> <p>Blurry images are common in high reconstruction loss weight as the MSELoss focuses on structure and color (where models commonly lean towards average pixel values), and fails to capture high-frequency details. The latent image's appears to be noisy as the codebook embeddings is disorganized and is used in a patch-by-patch manner in solving the pixel-matching puzzle instead of abstract features. Furthermore, the perceptual and adversarial losses are far too insignficiant for the model to weight importance on sharp and realistic textures. The model in this case is more of a pixel matcher than a image compressor preserving important information. </p>"},{"location":"report/#run-3-2025-06-05_05-12-17","title":"Run 3: 2025-06-05_05-12-17","text":"<p>Comparing the original image (top row), latent image (middle row), reconstructed image (bottom row) for run 3: 2025-06-05_05-12-17. Epoch: 10</p> Parameter Configurations (Run 3) <pre><code>datamodule:\n  batch_size: 16\nmodels:\n  autoencoder:\n    beta: 0.033203203889255975\n    down_channels: [16, 64, 128, 256]\n    embedding_dim: 6\n    num_down_layers: 2\n    num_mid_layers: 2\n    num_up_layers: 2\n    num_embeddings: 5377\n    num_heads: 32\n  discriminator:\n    conv_channels: [16, 32, 64]\nsystems:\n  loss_weights:\n    adversarial: 0.0002824064862999839\n    internal: 8.710494682214064\n    perceptual: 0.0002784928081396848\n    reconst: 0.5440532552186204\nlr:\n  d: 1.424399124491354e-06\n  g: 0.00010944209261365034\n</code></pre> <p>The results in run3 is the best so far. Despite the color imbalance, each reconstructed image maintains the structural and peceptual detail of the original. We may observe the codebook embeddings in the latent images holding less variance in values compared to run2 and run4, which implies the strength that is being applied to the codebook+commitment losses.</p> <p>The issue in run 3 is mainly the  imbalanaced loss weighting, where similar to run 1, the internal loss is the most dominant. The difference between the losses is not as severe in this case, allowing a more accurate sharp reconstruction, but primarily lacks in coloring and the reconstruction seems to be more faded out / brown-ish tinted. Indicating the reconstruction loss could be weighted more to focus on color. The dominant internal loss preoccupies the model to focus o n codeboook usage rather than the image quality. This configuration has a high codebook capacity with 5377 number of embeddings and a 32-headed attention mechanism which should effectively capture features as they do much better comparing the run 2. </p>"},{"location":"report/#run-4-2025-06-05_08-52-13","title":"Run 4: 2025-06-05_08-52-13","text":"<p>Comparing the original image (top row), latent image (middle row), reconstructed image (bottom row) for run 4: 2025-06-05_08-52-13. Epoch: 10</p> Parameter Configurations (Run 4) <pre><code>datamodule:\n  batch_size: 64\nmodels:\n  autoencoder:\n    beta: 0.6443345881327225\n    down_channels: [32, 64, 128, 256]\n    embedding_dim: 5\n    num_down_layers: 1\n    num_mid_layers: 4\n    num_up_layers: 2\n    num_embeddings: 4014\n    num_heads: 4\n  discriminator:\n    conv_channels: [64, 128, 256]\nsystems:\n  loss_weights:\n    adversarial: 0.7760262788835401\n    internal: 4.333458542618579\n    perceptual: 0.0012227879638211584\n    reconst: 65.10522954739554\nlr:\n  d: 9.227559277019527e-06\n  g: 0.0014256583051953369\n</code></pre> <p>Run4 had the best configuration out of the tested ones. We may also observe this from the comparison image above. The images produced substantially sharper and retains more details with accurate color reconstruction. The model is able to capture facial expressions, positions, and textural details like hair. The latent images appears to be vibrant and distinctly structured, indication good latent compression. Similar to run 3, the latent images visibly correspond to the input feature semantics (face, eyes, hair, and clothes).</p> <p>The reason of this apparent success is due to a good, balanced, and effective configuration of losses and model architecture. Even though the reconstruction loss is weighted high, strong adversarial weight contributed to ensure human-like features taking advantage and unravelling from what is learned by discriminator. </p>"},{"location":"report/#final-model","title":"Final Model","text":""},{"location":"report/#sweep-configuration","title":"Sweep Configuration","text":""},{"location":"report/#denoising-diffusion-probabilitic-model","title":"Denoising Diffusion Probabilitic Model","text":""},{"location":"report/#hyperparameter-optimization_1","title":"Hyperparameter Optimization","text":""},{"location":"report/#notable-sweep-configuration","title":"Notable Sweep Configuration","text":"<p>Out of the 30 or so run trials, I ran 2 of the best configurations up until 100 epochs. The results are the following.</p>"},{"location":"report/#run1-giddy-grass-43","title":"Run1: Giddy Grass 43","text":"Hyperparameter Configuration <pre><code>datamodule:\n  batch_size: 56\n  num_workers: 4\nddpm:\n  lr: 0.00013256608655142174\nmodels:\n  diffusion:\n    conv_out_channels: 352\n    down_channels:\n      - 256\n      - 384\n      - 512\n      - 768\n    mid_channels:\n      - 768\n      - 512\n    num_down_layers: 1\n    num_mid_layers: 2\n    num_up_layers: 4\n    num_heads: 2\n    t_emb_dim: 309\nnoise_scheduler:\n  beta_start: 0.00010505422393815704\n  beta_end: 0.06392103975835639\n  num_timesteps: 2617\n</code></pre>"},{"location":"report/#run2-eager-sound-42","title":"Run2: Eager Sound 42","text":"Hyperparameter Configuration <pre><code>datamodule:\n  batch_size: 25\n  num_workers: 4\nddpm:\n  lr: 7.016014614095296e-06\nmodels:\n  diffusion:\n    conv_out_channels: 336\n    down_channels:\n      - 128\n      - 256\n      - 512\n      - 768\n    mid_channels:\n      - 768\n      - 512\n      - 512\n    num_down_layers: 1\n    num_mid_layers: 4\n    num_up_layers: 1\n    num_heads: 4\n    t_emb_dim: 743\nnoise_scheduler:\n  beta_start: 0.000986132869834486\n  beta_end: 0.08828010703949035\n  num_timesteps: 929\n</code></pre>"},{"location":"report/#conclusion","title":"Conclusion","text":"<p>Oh my god I just noticed I needed to quantize the final output for the best final results. </p>"}]}